{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIST_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyQ+H4b21WpCza9ZtgUU4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olivia-Feldman/NUGAN-DISTGAN/blob/Olivia/DIST_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSigDUfUA0On"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch8zqg8bcneW"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb-qY4uZcprx"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqtgJep-ctzY"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XJXKI2sv2bx"
      },
      "source": [
        "def initialize_weights(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.ConvTranspose2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50yvRNhBdHuc"
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder,self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(True), nn.Linear(64, 12), \n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(12, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = self.encoder(x)\n",
        "     \n",
        "        \n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3tMaTYAdSJm"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, input_size=28, base_size=128):\n",
        "        super(Generator, self).__init__()  \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.input_size = 28\n",
        "        self.base_size = base_size\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_dim, self.base_size)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, self.input_size* self.input_size )\n",
        "        initialize_weights(self)                    \n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "       \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "       \n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, input_size=28, base_size=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.input_size = input_size\n",
        "        self.base_size = base_size\n",
        "\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear( self.input_size* self.input_size,self.base_size)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, output_dim)\n",
        "\n",
        "        initialize_weights(self)\n",
        "\n",
        "     # forward method\n",
        "  def forward(self, x):\n",
        "       # x = x.view(-1, self.input_size * self.input_size)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "       # x = x.view(-1, self.input_size * self.input_size)\n",
        "      \n",
        "        return torch.sigmoid(self.fc4(x))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "732MQHr1dU-4"
      },
      "source": [
        "\n",
        "def visualize_results(gan):\n",
        "      sample_z_ = torch.rand((10, gan.z_dim)).cuda()\n",
        "      samples = gan.G(sample_z_)\n",
        "      samples = samples.cpu().data.numpy().reshape(10,28,28,1)\n",
        "      samples = (samples + 1) / 2\n",
        "      plt.figure(figsize=((1,10)))\n",
        "      fig,ax = plt.subplots(1,10)\n",
        "      for i in range(10):\n",
        "        s=ax[i].imshow(np.squeeze(samples[i,]))\n",
        "        s=ax[i].get_xaxis().set_visible(False)\n",
        "        s=ax[i].get_yaxis().set_visible(False)\n",
        "      s=plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBPFiScdbXy"
      },
      "source": [
        "\n",
        "class GAN():\n",
        "    def __init__(self,params):\n",
        "        # parameters\n",
        "        self.epoch = params['max_epochs']\n",
        "        self.sample_num = 100\n",
        "        self.batch_size = params['base_size']\n",
        "        self.input_size = 28\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.base_size = params['base_size']\n",
        "\n",
        "        # load dataset\n",
        "        self.data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=self.batch_size, \n",
        "                                               shuffle=True)\n",
        "        data = self.data_loader.__iter__().__next__()[0]\n",
        "\n",
        "        #print(data.shape[0])\n",
        "\n",
        "        # initialization of the generator and discriminator\n",
        "        \n",
        "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[0], input_size=self.input_size,base_size=self.base_size).cuda()\n",
        "        self.D = Discriminator(input_dim=data.shape[0], output_dim=1, input_size=self.input_size,base_size=self.base_size).cuda()\n",
        "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']),eps=1e-09)\n",
        "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']),eps=1e-09)\n",
        "        \n",
        "        # initialization of the loss function\n",
        "        #self.BCE_loss = nn.SmoothL1Loss(beta=0.9)\n",
        "        self.BCE_loss = nn.BCELoss().cuda()\n",
        "        self.BCE_loss = nn.MSELoss()\n",
        "        \n",
        "        # Gettng a batch of noise to generate the fake data\n",
        "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim)).cuda()\n",
        "        \n",
        "# Fucntion to train the GAN, where you alternate between the training of the genenator and discriminator\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "       # Setting empty arrays for storing the losses\n",
        "\n",
        "        self.train_hist = {}\n",
        "        self.train_hist['D_loss'] = []\n",
        "        self.train_hist['G_loss'] = []\n",
        "\n",
        "        # Setting up the labels for real and fake images\n",
        "        self.y_real_, self.y_fake_ = torch.ones(self.batch_size,1).fill_(0.9).type(torch.float32).cuda(), torch.zeros(self.batch_size, 1).cuda()\n",
        "        \n",
        "        print('training start!!')\n",
        "\n",
        "        # Epoch loops\n",
        "\n",
        "        for epoch in range(self.epoch):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "\n",
        "            for iter, (x_, _) in enumerate(self.data_loader):\n",
        "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
        "                    break\n",
        "\n",
        "                # Generate random noise to push through the generator   \n",
        "\n",
        "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
        "                x_, z_ = x_, z_.cuda()\n",
        "                \n",
        "                x_ = Variable(x_).cuda(device).type(torch.cuda.FloatTensor)\n",
        "                x_= x_.view(x_.size(0), -1)\n",
        "                \n",
        "\n",
        "               \n",
        "\n",
        "                # update D network using \n",
        "                # 1. Set optimizer gradient to zero\n",
        "                gan.D_optimizer.zero_grad()\n",
        "                \n",
        "                # 2. Set discriminator losses on real and fake data\n",
        "\n",
        "                # training the encoder \n",
        "                encoder = autoencoder().cuda()\n",
        "\n",
        "                # encoder is used for latent_variable mapping c\n",
        "                recon =  encoder(x_)\n",
        "\n",
        "\n",
        "                recon_real = gan.G(recon)\n",
        "                recon_fake = gan.G(z_)\n",
        "\n",
        "\n",
        "                #print(recon_fake.shape)\n",
        "                D_real = gan.D(recon_real)\n",
        "                #print(D_real.shape)\n",
        "\n",
        "                \n",
        "                D_real_loss = gan.BCE_loss(D_real,gan.y_real_)\n",
        "                \n",
        "                D_fake = gan.D(recon_fake)\n",
        "                D_fake_loss = gan.BCE_loss(D_fake, gan.y_fake_)\n",
        "\n",
        "                D_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                D_loss.backward()\n",
        "                # 4. Make a step of D_optimizer\n",
        "                gan.D_optimizer.step()\n",
        "\n",
        "                # 5. Set the current loss in self.train_hist['D_loss]\n",
        "                gan.train_hist['D_loss'].append(D_loss.item())\n",
        "                \n",
        "                # update G network using \n",
        "                # 1. Set optimizer gradient to zero\n",
        "                gan.G_optimizer.zero_grad()\n",
        "                # 2. Set generator losses on fake data\n",
        "               \n",
        "                recon_fake = gan.G(z_)\n",
        "                D_fake = gan.D(recon_fake)\n",
        "                G_loss = gan.BCE_loss(D_fake, gan.y_real_)\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                G_loss.backward()\n",
        "                # 4. Make a step of G_optimizer\n",
        "                gan.G_optimizer.step()\n",
        "                # 5. Set the current loss in self.train_hist['G_loss]    \n",
        "                gan.train_hist['G_loss'].append(G_loss.item())\n",
        "\n",
        "                # Print iterations and losses\n",
        "\n",
        "                if ((iter + 1) % 50) == 0:\n",
        "                  print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
        "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
        "    \n",
        "                  \n",
        "            # Visualize results\n",
        "                with torch.no_grad():\n",
        "                 visualize_results(self)\n",
        "        #plt.figure(figsize=(16,8))\n",
        "       # s=plt.plot(gan.train_hist['D_loss'],c='b')\n",
        "        #s=plt.plot(gan.train_hist['G_loss'],c='r')\n",
        "        #s = plt.ylim((0,1))\n",
        "        #s = plt.grid()\n",
        "       # s=plt.legend(('Discriminator loss','Generator loss'))\n",
        "\n",
        "        print(\"Training finished!\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEXhP73SyT2m"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW-BInEbtLlE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c991dfd2-6809-408c-a720-3c151028ceee"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "params = {'beta1': 0.05, 'beta2': 0.999,'lr_g':0.0002,'lr_d':0.0002,'max_epochs':30}\n",
        "params['z_dim'] =2\n",
        "params['base_size'] = 128\n",
        "\n",
        "gan = GAN(params)\n",
        "\n",
        "\n",
        "\n",
        "gan.train()\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start!!\n",
            "Epoch: [ 1] [  50/ 468] D_loss: 0.40878743, G_loss: 0.16545634\n",
            "Epoch: [ 1] [ 100/ 468] D_loss: 0.40550172, G_loss: 0.19056547\n",
            "Epoch: [ 1] [ 150/ 468] D_loss: 0.40504295, G_loss: 0.19927353\n",
            "Epoch: [ 1] [ 200/ 468] D_loss: 0.40643799, G_loss: 0.20109692\n",
            "Epoch: [ 1] [ 250/ 468] D_loss: 0.40502951, G_loss: 0.20286672\n",
            "Epoch: [ 1] [ 300/ 468] D_loss: 0.40509033, G_loss: 0.20182551\n",
            "Epoch: [ 1] [ 350/ 468] D_loss: 0.40475231, G_loss: 0.20526439\n",
            "Epoch: [ 1] [ 400/ 468] D_loss: 0.40593392, G_loss: 0.19875921\n",
            "Epoch: [ 1] [ 450/ 468] D_loss: 0.40696591, G_loss: 0.20211640\n",
            "Epoch: [ 2] [  50/ 468] D_loss: 0.40484846, G_loss: 0.19926845\n",
            "Epoch: [ 2] [ 100/ 468] D_loss: 0.40554494, G_loss: 0.20057660\n",
            "Epoch: [ 2] [ 150/ 468] D_loss: 0.40428355, G_loss: 0.20593821\n",
            "Epoch: [ 2] [ 200/ 468] D_loss: 0.40446559, G_loss: 0.20218173\n",
            "Epoch: [ 2] [ 250/ 468] D_loss: 0.40635186, G_loss: 0.20379253\n",
            "Epoch: [ 2] [ 300/ 468] D_loss: 0.40407097, G_loss: 0.20556369\n",
            "Epoch: [ 2] [ 350/ 468] D_loss: 0.40607727, G_loss: 0.20329015\n",
            "Epoch: [ 2] [ 400/ 468] D_loss: 0.40477008, G_loss: 0.20744550\n",
            "Epoch: [ 2] [ 450/ 468] D_loss: 0.40473336, G_loss: 0.20470479\n",
            "Epoch: [ 3] [  50/ 468] D_loss: 0.40584773, G_loss: 0.19926137\n",
            "Epoch: [ 3] [ 100/ 468] D_loss: 0.40424079, G_loss: 0.20528717\n",
            "Epoch: [ 3] [ 150/ 468] D_loss: 0.40535766, G_loss: 0.20140520\n",
            "Epoch: [ 3] [ 200/ 468] D_loss: 0.40665442, G_loss: 0.19990611\n",
            "Epoch: [ 3] [ 250/ 468] D_loss: 0.40476102, G_loss: 0.20102759\n",
            "Epoch: [ 3] [ 300/ 468] D_loss: 0.40424150, G_loss: 0.20479645\n",
            "Epoch: [ 3] [ 350/ 468] D_loss: 0.40508580, G_loss: 0.20037004\n",
            "Epoch: [ 3] [ 400/ 468] D_loss: 0.40524599, G_loss: 0.20059271\n",
            "Epoch: [ 3] [ 450/ 468] D_loss: 0.40556335, G_loss: 0.19810459\n",
            "Epoch: [ 4] [  50/ 468] D_loss: 0.40483373, G_loss: 0.20283875\n",
            "Epoch: [ 4] [ 100/ 468] D_loss: 0.40459207, G_loss: 0.20490006\n",
            "Epoch: [ 4] [ 150/ 468] D_loss: 0.40501684, G_loss: 0.20570509\n",
            "Epoch: [ 4] [ 200/ 468] D_loss: 0.40467674, G_loss: 0.20502880\n",
            "Epoch: [ 4] [ 250/ 468] D_loss: 0.40514189, G_loss: 0.20524961\n",
            "Epoch: [ 4] [ 300/ 468] D_loss: 0.40528327, G_loss: 0.20449123\n",
            "Epoch: [ 4] [ 350/ 468] D_loss: 0.40493512, G_loss: 0.20270362\n",
            "Epoch: [ 4] [ 400/ 468] D_loss: 0.40418532, G_loss: 0.20678416\n",
            "Epoch: [ 4] [ 450/ 468] D_loss: 0.40524846, G_loss: 0.20080504\n",
            "Epoch: [ 5] [  50/ 468] D_loss: 0.40679184, G_loss: 0.20165521\n",
            "Epoch: [ 5] [ 100/ 468] D_loss: 0.40461728, G_loss: 0.20464745\n",
            "Epoch: [ 5] [ 150/ 468] D_loss: 0.40478832, G_loss: 0.20429939\n",
            "Epoch: [ 5] [ 200/ 468] D_loss: 0.40473986, G_loss: 0.20270491\n",
            "Epoch: [ 5] [ 250/ 468] D_loss: 0.40403792, G_loss: 0.20537348\n",
            "Epoch: [ 5] [ 300/ 468] D_loss: 0.40452793, G_loss: 0.20300776\n",
            "Epoch: [ 5] [ 350/ 468] D_loss: 0.40687472, G_loss: 0.20206481\n",
            "Epoch: [ 5] [ 400/ 468] D_loss: 0.40488377, G_loss: 0.20221177\n",
            "Epoch: [ 5] [ 450/ 468] D_loss: 0.40618637, G_loss: 0.20303558\n",
            "Epoch: [ 6] [  50/ 468] D_loss: 0.40510029, G_loss: 0.20025758\n",
            "Epoch: [ 6] [ 100/ 468] D_loss: 0.40470746, G_loss: 0.20347694\n",
            "Epoch: [ 6] [ 150/ 468] D_loss: 0.40563014, G_loss: 0.20075807\n",
            "Epoch: [ 6] [ 200/ 468] D_loss: 0.40483677, G_loss: 0.20780540\n",
            "Epoch: [ 6] [ 250/ 468] D_loss: 0.40545428, G_loss: 0.20157887\n",
            "Epoch: [ 6] [ 300/ 468] D_loss: 0.40512592, G_loss: 0.20001543\n",
            "Epoch: [ 6] [ 350/ 468] D_loss: 0.40511689, G_loss: 0.20261537\n",
            "Epoch: [ 6] [ 400/ 468] D_loss: 0.40493190, G_loss: 0.20533967\n",
            "Epoch: [ 6] [ 450/ 468] D_loss: 0.40503198, G_loss: 0.20116505\n",
            "Epoch: [ 7] [  50/ 468] D_loss: 0.40462416, G_loss: 0.20566747\n",
            "Epoch: [ 7] [ 100/ 468] D_loss: 0.40522438, G_loss: 0.20178148\n",
            "Epoch: [ 7] [ 150/ 468] D_loss: 0.40512618, G_loss: 0.20517924\n",
            "Epoch: [ 7] [ 200/ 468] D_loss: 0.40474713, G_loss: 0.20002913\n",
            "Epoch: [ 7] [ 250/ 468] D_loss: 0.40564311, G_loss: 0.20099887\n",
            "Epoch: [ 7] [ 300/ 468] D_loss: 0.40620643, G_loss: 0.20118953\n",
            "Epoch: [ 7] [ 350/ 468] D_loss: 0.40488780, G_loss: 0.20119196\n",
            "Epoch: [ 7] [ 400/ 468] D_loss: 0.40506318, G_loss: 0.20119280\n",
            "Epoch: [ 7] [ 450/ 468] D_loss: 0.40555155, G_loss: 0.20140207\n",
            "Epoch: [ 8] [  50/ 468] D_loss: 0.40527713, G_loss: 0.20146793\n",
            "Epoch: [ 8] [ 100/ 468] D_loss: 0.40491778, G_loss: 0.20131747\n",
            "Epoch: [ 8] [ 150/ 468] D_loss: 0.40492332, G_loss: 0.20268875\n",
            "Epoch: [ 8] [ 200/ 468] D_loss: 0.40523189, G_loss: 0.20223460\n",
            "Epoch: [ 8] [ 250/ 468] D_loss: 0.40588540, G_loss: 0.20315191\n",
            "Epoch: [ 8] [ 300/ 468] D_loss: 0.40449753, G_loss: 0.20434760\n",
            "Epoch: [ 8] [ 350/ 468] D_loss: 0.40481943, G_loss: 0.20346144\n",
            "Epoch: [ 8] [ 400/ 468] D_loss: 0.40590540, G_loss: 0.20124881\n",
            "Epoch: [ 8] [ 450/ 468] D_loss: 0.40465686, G_loss: 0.20529184\n",
            "Epoch: [ 9] [  50/ 468] D_loss: 0.40522599, G_loss: 0.20277259\n",
            "Epoch: [ 9] [ 100/ 468] D_loss: 0.40523803, G_loss: 0.20189153\n",
            "Epoch: [ 9] [ 150/ 468] D_loss: 0.40489829, G_loss: 0.20241116\n",
            "Epoch: [ 9] [ 200/ 468] D_loss: 0.40517259, G_loss: 0.20176245\n",
            "Epoch: [ 9] [ 250/ 468] D_loss: 0.40504536, G_loss: 0.20158049\n",
            "Epoch: [ 9] [ 300/ 468] D_loss: 0.40454152, G_loss: 0.20386592\n",
            "Epoch: [ 9] [ 350/ 468] D_loss: 0.40491998, G_loss: 0.20130363\n",
            "Epoch: [ 9] [ 400/ 468] D_loss: 0.40559340, G_loss: 0.20105498\n",
            "Epoch: [ 9] [ 450/ 468] D_loss: 0.40499282, G_loss: 0.20147175\n",
            "Epoch: [10] [  50/ 468] D_loss: 0.40487680, G_loss: 0.20368502\n",
            "Epoch: [10] [ 100/ 468] D_loss: 0.40537399, G_loss: 0.20171219\n",
            "Epoch: [10] [ 150/ 468] D_loss: 0.40452403, G_loss: 0.20368898\n",
            "Epoch: [10] [ 200/ 468] D_loss: 0.40473527, G_loss: 0.20246908\n",
            "Epoch: [10] [ 250/ 468] D_loss: 0.40524966, G_loss: 0.20159286\n",
            "Epoch: [10] [ 300/ 468] D_loss: 0.40516818, G_loss: 0.20386827\n",
            "Epoch: [10] [ 350/ 468] D_loss: 0.40481150, G_loss: 0.20201065\n",
            "Epoch: [10] [ 400/ 468] D_loss: 0.40503949, G_loss: 0.20371988\n",
            "Epoch: [10] [ 450/ 468] D_loss: 0.40490898, G_loss: 0.20306152\n",
            "Epoch: [11] [  50/ 468] D_loss: 0.40530777, G_loss: 0.20142382\n",
            "Epoch: [11] [ 100/ 468] D_loss: 0.40505475, G_loss: 0.20200351\n",
            "Epoch: [11] [ 150/ 468] D_loss: 0.40478146, G_loss: 0.20325109\n",
            "Epoch: [11] [ 200/ 468] D_loss: 0.40475670, G_loss: 0.20429219\n",
            "Epoch: [11] [ 250/ 468] D_loss: 0.40491736, G_loss: 0.20161830\n",
            "Epoch: [11] [ 300/ 468] D_loss: 0.40506789, G_loss: 0.20296545\n",
            "Epoch: [11] [ 350/ 468] D_loss: 0.40496415, G_loss: 0.20194244\n",
            "Epoch: [11] [ 400/ 468] D_loss: 0.40542135, G_loss: 0.20331272\n",
            "Epoch: [11] [ 450/ 468] D_loss: 0.40472090, G_loss: 0.20264792\n",
            "Epoch: [12] [  50/ 468] D_loss: 0.40525737, G_loss: 0.20363078\n",
            "Epoch: [12] [ 100/ 468] D_loss: 0.40490681, G_loss: 0.20291059\n",
            "Epoch: [12] [ 150/ 468] D_loss: 0.40520892, G_loss: 0.20139775\n",
            "Epoch: [12] [ 200/ 468] D_loss: 0.40498737, G_loss: 0.20197612\n",
            "Epoch: [12] [ 250/ 468] D_loss: 0.40521196, G_loss: 0.20143682\n",
            "Epoch: [12] [ 300/ 468] D_loss: 0.40510920, G_loss: 0.20293657\n",
            "Epoch: [12] [ 350/ 468] D_loss: 0.40516907, G_loss: 0.20284209\n",
            "Epoch: [12] [ 400/ 468] D_loss: 0.40470478, G_loss: 0.20400243\n",
            "Epoch: [12] [ 450/ 468] D_loss: 0.40492129, G_loss: 0.20269094\n",
            "Epoch: [13] [  50/ 468] D_loss: 0.40494335, G_loss: 0.20356873\n",
            "Epoch: [13] [ 100/ 468] D_loss: 0.40523100, G_loss: 0.20281100\n",
            "Epoch: [13] [ 150/ 468] D_loss: 0.40495473, G_loss: 0.20264129\n",
            "Epoch: [13] [ 200/ 468] D_loss: 0.40481430, G_loss: 0.20349073\n",
            "Epoch: [13] [ 250/ 468] D_loss: 0.40519494, G_loss: 0.20377445\n",
            "Epoch: [13] [ 300/ 468] D_loss: 0.40457755, G_loss: 0.20345330\n",
            "Epoch: [13] [ 350/ 468] D_loss: 0.40500885, G_loss: 0.20253713\n",
            "Epoch: [13] [ 400/ 468] D_loss: 0.40524912, G_loss: 0.20266591\n",
            "Epoch: [13] [ 450/ 468] D_loss: 0.40491489, G_loss: 0.20401496\n",
            "Epoch: [14] [  50/ 468] D_loss: 0.40531635, G_loss: 0.20281041\n",
            "Epoch: [14] [ 100/ 468] D_loss: 0.40513030, G_loss: 0.20363040\n",
            "Epoch: [14] [ 150/ 468] D_loss: 0.40477979, G_loss: 0.20352316\n",
            "Epoch: [14] [ 200/ 468] D_loss: 0.40469241, G_loss: 0.20209920\n",
            "Epoch: [14] [ 250/ 468] D_loss: 0.40500578, G_loss: 0.20179746\n",
            "Epoch: [14] [ 300/ 468] D_loss: 0.40473783, G_loss: 0.20282039\n",
            "Epoch: [14] [ 350/ 468] D_loss: 0.40530610, G_loss: 0.20271133\n",
            "Epoch: [14] [ 400/ 468] D_loss: 0.40494889, G_loss: 0.20271960\n",
            "Epoch: [14] [ 450/ 468] D_loss: 0.40523419, G_loss: 0.20172974\n",
            "Epoch: [15] [  50/ 468] D_loss: 0.40531504, G_loss: 0.20245007\n",
            "Epoch: [15] [ 100/ 468] D_loss: 0.40472865, G_loss: 0.20289451\n",
            "Epoch: [15] [ 150/ 468] D_loss: 0.40513259, G_loss: 0.20233172\n",
            "Epoch: [15] [ 200/ 468] D_loss: 0.40537199, G_loss: 0.20279369\n",
            "Epoch: [15] [ 250/ 468] D_loss: 0.40507585, G_loss: 0.20206037\n",
            "Epoch: [15] [ 300/ 468] D_loss: 0.40475130, G_loss: 0.20292360\n",
            "Epoch: [15] [ 350/ 468] D_loss: 0.40489256, G_loss: 0.20246878\n",
            "Epoch: [15] [ 400/ 468] D_loss: 0.40484369, G_loss: 0.20258364\n",
            "Epoch: [15] [ 450/ 468] D_loss: 0.40504158, G_loss: 0.20276037\n",
            "Epoch: [16] [  50/ 468] D_loss: 0.40513331, G_loss: 0.20258093\n",
            "Epoch: [16] [ 100/ 468] D_loss: 0.40511325, G_loss: 0.20249091\n",
            "Epoch: [16] [ 150/ 468] D_loss: 0.40477222, G_loss: 0.20306039\n",
            "Epoch: [16] [ 200/ 468] D_loss: 0.40478346, G_loss: 0.20234644\n",
            "Epoch: [16] [ 250/ 468] D_loss: 0.40501064, G_loss: 0.20236948\n",
            "Epoch: [16] [ 300/ 468] D_loss: 0.40504152, G_loss: 0.20231916\n",
            "Epoch: [16] [ 350/ 468] D_loss: 0.40504223, G_loss: 0.20263633\n",
            "Epoch: [16] [ 400/ 468] D_loss: 0.40512159, G_loss: 0.20265636\n",
            "Epoch: [16] [ 450/ 468] D_loss: 0.40494362, G_loss: 0.20233229\n",
            "Epoch: [17] [  50/ 468] D_loss: 0.40516075, G_loss: 0.20216575\n",
            "Epoch: [17] [ 100/ 468] D_loss: 0.40495044, G_loss: 0.20250380\n",
            "Epoch: [17] [ 150/ 468] D_loss: 0.40489560, G_loss: 0.20245282\n",
            "Epoch: [17] [ 200/ 468] D_loss: 0.40506676, G_loss: 0.20260115\n",
            "Epoch: [17] [ 250/ 468] D_loss: 0.40502068, G_loss: 0.20252711\n",
            "Epoch: [17] [ 300/ 468] D_loss: 0.40508765, G_loss: 0.20236568\n",
            "Epoch: [17] [ 350/ 468] D_loss: 0.40502539, G_loss: 0.20250235\n",
            "Epoch: [17] [ 400/ 468] D_loss: 0.40498745, G_loss: 0.20249578\n",
            "Epoch: [17] [ 450/ 468] D_loss: 0.40511551, G_loss: 0.20246005\n",
            "Epoch: [18] [  50/ 468] D_loss: 0.40479982, G_loss: 0.20243412\n",
            "Epoch: [18] [ 100/ 468] D_loss: 0.40514773, G_loss: 0.20229812\n",
            "Epoch: [18] [ 150/ 468] D_loss: 0.40488824, G_loss: 0.20225200\n",
            "Epoch: [18] [ 200/ 468] D_loss: 0.40503606, G_loss: 0.20266727\n",
            "Epoch: [18] [ 250/ 468] D_loss: 0.40511021, G_loss: 0.20215864\n",
            "Epoch: [18] [ 300/ 468] D_loss: 0.40518239, G_loss: 0.20263663\n",
            "Epoch: [18] [ 350/ 468] D_loss: 0.40483007, G_loss: 0.20224462\n",
            "Epoch: [18] [ 400/ 468] D_loss: 0.40501314, G_loss: 0.20246348\n",
            "Epoch: [18] [ 450/ 468] D_loss: 0.40514439, G_loss: 0.20250757\n",
            "Epoch: [19] [  50/ 468] D_loss: 0.40502620, G_loss: 0.20281819\n",
            "Epoch: [19] [ 100/ 468] D_loss: 0.40508908, G_loss: 0.20242408\n",
            "Epoch: [19] [ 150/ 468] D_loss: 0.40516576, G_loss: 0.20236410\n",
            "Epoch: [19] [ 200/ 468] D_loss: 0.40519118, G_loss: 0.20277102\n",
            "Epoch: [19] [ 250/ 468] D_loss: 0.40502110, G_loss: 0.20277314\n",
            "Epoch: [19] [ 300/ 468] D_loss: 0.40508872, G_loss: 0.20264369\n",
            "Epoch: [19] [ 350/ 468] D_loss: 0.40520966, G_loss: 0.20254743\n",
            "Epoch: [19] [ 400/ 468] D_loss: 0.40495315, G_loss: 0.20355780\n",
            "Epoch: [19] [ 450/ 468] D_loss: 0.40494341, G_loss: 0.20317045\n",
            "Epoch: [20] [  50/ 468] D_loss: 0.40506846, G_loss: 0.20158818\n",
            "Epoch: [20] [ 100/ 468] D_loss: 0.40540871, G_loss: 0.20248985\n",
            "Epoch: [20] [ 150/ 468] D_loss: 0.40513429, G_loss: 0.20240243\n",
            "Epoch: [20] [ 200/ 468] D_loss: 0.40511397, G_loss: 0.20137355\n",
            "Epoch: [20] [ 250/ 468] D_loss: 0.40532953, G_loss: 0.20169663\n",
            "Epoch: [20] [ 300/ 468] D_loss: 0.40501824, G_loss: 0.20307583\n",
            "Epoch: [20] [ 350/ 468] D_loss: 0.40504807, G_loss: 0.20208716\n",
            "Epoch: [20] [ 400/ 468] D_loss: 0.40497002, G_loss: 0.20259595\n",
            "Epoch: [20] [ 450/ 468] D_loss: 0.40522677, G_loss: 0.20264703\n",
            "Epoch: [21] [  50/ 468] D_loss: 0.40498123, G_loss: 0.20251389\n",
            "Epoch: [21] [ 100/ 468] D_loss: 0.40504608, G_loss: 0.20256498\n",
            "Epoch: [21] [ 150/ 468] D_loss: 0.40491617, G_loss: 0.20249596\n",
            "Epoch: [21] [ 200/ 468] D_loss: 0.40498456, G_loss: 0.20356591\n",
            "Epoch: [21] [ 250/ 468] D_loss: 0.40502059, G_loss: 0.20223337\n",
            "Epoch: [21] [ 300/ 468] D_loss: 0.40513059, G_loss: 0.20226070\n",
            "Epoch: [21] [ 350/ 468] D_loss: 0.40488753, G_loss: 0.20259127\n",
            "Epoch: [21] [ 400/ 468] D_loss: 0.40512043, G_loss: 0.20253459\n",
            "Epoch: [21] [ 450/ 468] D_loss: 0.40489382, G_loss: 0.20276886\n",
            "Epoch: [22] [  50/ 468] D_loss: 0.40480047, G_loss: 0.20245329\n",
            "Epoch: [22] [ 100/ 468] D_loss: 0.40501845, G_loss: 0.20254532\n",
            "Epoch: [22] [ 150/ 468] D_loss: 0.40514821, G_loss: 0.20254467\n",
            "Epoch: [22] [ 200/ 468] D_loss: 0.40489119, G_loss: 0.20241307\n",
            "Epoch: [22] [ 250/ 468] D_loss: 0.40500799, G_loss: 0.20251618\n",
            "Epoch: [22] [ 300/ 468] D_loss: 0.40513137, G_loss: 0.20234597\n",
            "Epoch: [22] [ 350/ 468] D_loss: 0.40516597, G_loss: 0.20228088\n",
            "Epoch: [22] [ 400/ 468] D_loss: 0.40514073, G_loss: 0.20246717\n",
            "Epoch: [22] [ 450/ 468] D_loss: 0.40507829, G_loss: 0.20234604\n",
            "Epoch: [23] [  50/ 468] D_loss: 0.40499574, G_loss: 0.20243675\n",
            "Epoch: [23] [ 100/ 468] D_loss: 0.40490013, G_loss: 0.20244101\n",
            "Epoch: [23] [ 150/ 468] D_loss: 0.40497264, G_loss: 0.20255363\n",
            "Epoch: [23] [ 200/ 468] D_loss: 0.40511554, G_loss: 0.20266291\n",
            "Epoch: [23] [ 250/ 468] D_loss: 0.40509170, G_loss: 0.20235091\n",
            "Epoch: [23] [ 300/ 468] D_loss: 0.40515649, G_loss: 0.20233081\n",
            "Epoch: [23] [ 350/ 468] D_loss: 0.40506941, G_loss: 0.20240223\n",
            "Epoch: [23] [ 400/ 468] D_loss: 0.40495539, G_loss: 0.20236871\n",
            "Epoch: [23] [ 450/ 468] D_loss: 0.40495786, G_loss: 0.20241785\n",
            "Epoch: [24] [  50/ 468] D_loss: 0.40493581, G_loss: 0.20224488\n",
            "Epoch: [24] [ 100/ 468] D_loss: 0.40492272, G_loss: 0.20341820\n",
            "Epoch: [24] [ 150/ 468] D_loss: 0.40501139, G_loss: 0.20263214\n",
            "Epoch: [24] [ 200/ 468] D_loss: 0.40490669, G_loss: 0.20249882\n",
            "Epoch: [24] [ 250/ 468] D_loss: 0.40499663, G_loss: 0.20223683\n",
            "Epoch: [24] [ 300/ 468] D_loss: 0.40515548, G_loss: 0.20240530\n",
            "Epoch: [24] [ 350/ 468] D_loss: 0.40489131, G_loss: 0.20283943\n",
            "Epoch: [24] [ 400/ 468] D_loss: 0.40506774, G_loss: 0.20252186\n",
            "Epoch: [24] [ 450/ 468] D_loss: 0.40504318, G_loss: 0.20266689\n",
            "Epoch: [25] [  50/ 468] D_loss: 0.40500358, G_loss: 0.20249505\n",
            "Epoch: [25] [ 100/ 468] D_loss: 0.40466863, G_loss: 0.20267862\n",
            "Epoch: [25] [ 150/ 468] D_loss: 0.40506798, G_loss: 0.20244023\n",
            "Epoch: [25] [ 200/ 468] D_loss: 0.40535286, G_loss: 0.20240904\n",
            "Epoch: [25] [ 250/ 468] D_loss: 0.40476009, G_loss: 0.20241815\n",
            "Epoch: [25] [ 300/ 468] D_loss: 0.40508619, G_loss: 0.20254442\n",
            "Epoch: [25] [ 350/ 468] D_loss: 0.40484393, G_loss: 0.20367180\n",
            "Epoch: [25] [ 400/ 468] D_loss: 0.40526676, G_loss: 0.20211680\n",
            "Epoch: [25] [ 450/ 468] D_loss: 0.40513024, G_loss: 0.20219235\n",
            "Epoch: [26] [  50/ 468] D_loss: 0.40507272, G_loss: 0.20241816\n",
            "Epoch: [26] [ 100/ 468] D_loss: 0.40485597, G_loss: 0.20237526\n",
            "Epoch: [26] [ 150/ 468] D_loss: 0.40514356, G_loss: 0.20236394\n",
            "Epoch: [26] [ 200/ 468] D_loss: 0.40489751, G_loss: 0.20238519\n",
            "Epoch: [26] [ 250/ 468] D_loss: 0.40500572, G_loss: 0.20255126\n",
            "Epoch: [26] [ 300/ 468] D_loss: 0.40501398, G_loss: 0.20217046\n",
            "Epoch: [26] [ 350/ 468] D_loss: 0.40486255, G_loss: 0.20233953\n",
            "Epoch: [26] [ 400/ 468] D_loss: 0.40503240, G_loss: 0.20252913\n",
            "Epoch: [26] [ 450/ 468] D_loss: 0.40504488, G_loss: 0.20265433\n",
            "Epoch: [27] [  50/ 468] D_loss: 0.40475535, G_loss: 0.20259598\n",
            "Epoch: [27] [ 100/ 468] D_loss: 0.40503088, G_loss: 0.20241874\n",
            "Epoch: [27] [ 150/ 468] D_loss: 0.40496248, G_loss: 0.20258120\n",
            "Epoch: [27] [ 200/ 468] D_loss: 0.40503061, G_loss: 0.20292000\n",
            "Epoch: [27] [ 250/ 468] D_loss: 0.40477455, G_loss: 0.20267224\n",
            "Epoch: [27] [ 300/ 468] D_loss: 0.40496677, G_loss: 0.20256159\n",
            "Epoch: [27] [ 350/ 468] D_loss: 0.40498859, G_loss: 0.20268853\n",
            "Epoch: [27] [ 400/ 468] D_loss: 0.40504006, G_loss: 0.20242874\n",
            "Epoch: [27] [ 450/ 468] D_loss: 0.40490064, G_loss: 0.20266117\n",
            "Epoch: [28] [  50/ 468] D_loss: 0.40503699, G_loss: 0.20241940\n",
            "Epoch: [28] [ 100/ 468] D_loss: 0.40486753, G_loss: 0.20248562\n",
            "Epoch: [28] [ 150/ 468] D_loss: 0.40492979, G_loss: 0.20263124\n",
            "Epoch: [28] [ 200/ 468] D_loss: 0.40498707, G_loss: 0.20247400\n",
            "Epoch: [28] [ 250/ 468] D_loss: 0.40501726, G_loss: 0.20247746\n",
            "Epoch: [28] [ 300/ 468] D_loss: 0.40506575, G_loss: 0.20235814\n",
            "Epoch: [28] [ 350/ 468] D_loss: 0.40501186, G_loss: 0.20251940\n",
            "Epoch: [28] [ 400/ 468] D_loss: 0.40512699, G_loss: 0.20245291\n",
            "Epoch: [28] [ 450/ 468] D_loss: 0.40501061, G_loss: 0.20296362\n",
            "Epoch: [29] [  50/ 468] D_loss: 0.40495515, G_loss: 0.20269355\n",
            "Epoch: [29] [ 100/ 468] D_loss: 0.40493366, G_loss: 0.20261234\n",
            "Epoch: [29] [ 150/ 468] D_loss: 0.40498045, G_loss: 0.20244978\n",
            "Epoch: [29] [ 200/ 468] D_loss: 0.40490431, G_loss: 0.20250475\n",
            "Epoch: [29] [ 250/ 468] D_loss: 0.40498871, G_loss: 0.20256355\n",
            "Epoch: [29] [ 300/ 468] D_loss: 0.40512756, G_loss: 0.20260444\n",
            "Epoch: [29] [ 350/ 468] D_loss: 0.40499675, G_loss: 0.20299432\n",
            "Epoch: [29] [ 400/ 468] D_loss: 0.40498912, G_loss: 0.20247060\n",
            "Epoch: [29] [ 450/ 468] D_loss: 0.40488988, G_loss: 0.20257074\n",
            "Epoch: [30] [  50/ 468] D_loss: 0.40494710, G_loss: 0.20239061\n",
            "Epoch: [30] [ 100/ 468] D_loss: 0.40490308, G_loss: 0.20254551\n",
            "Epoch: [30] [ 150/ 468] D_loss: 0.40502673, G_loss: 0.20250854\n",
            "Epoch: [30] [ 200/ 468] D_loss: 0.40500244, G_loss: 0.20236951\n",
            "Epoch: [30] [ 250/ 468] D_loss: 0.40502772, G_loss: 0.20255315\n",
            "Epoch: [30] [ 300/ 468] D_loss: 0.40504217, G_loss: 0.20247062\n",
            "Epoch: [30] [ 350/ 468] D_loss: 0.40498942, G_loss: 0.20259142\n",
            "Epoch: [30] [ 400/ 468] D_loss: 0.40498394, G_loss: 0.20250997\n",
            "Epoch: [30] [ 450/ 468] D_loss: 0.40505373, G_loss: 0.20270786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 72x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAAuCAYAAAAWRMPkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYXUlEQVR4nO2da3Bc532fn/ecs/cbdgEQFwIEeJV40c0SZUlMI8sSacmO7KgXz8Sp2yRtZvqhteNM00w702Yy07SdSccdpZl+atzMZNpOrYzdtLGdsWRToklJtGuRIiVSvEkEQZAAFou9Y2/nnLcfDkBgL1ics1hcJJ/fF34hFs887+//4uDdcxZCSokbN27cuNn8KFsN4MaNGze/qHE3YDdu3LjZorgbsBs3btxsUdwN2I0bN262KO4G7MaNGzdbFHcDduPGjZstiubkP6vhkPTEEkgNPHmgr0atqqEVBEaPgQCEkCgpDT0IgXCZhYoPUREASA0Ur4GpK2AKlCpIBZQaGH4QXhMphfUaOYVKaR6jUBRbyaHmFMpd4ghGyhTLW8UBSkrt3IcpEEp312VrfXyyeupydNjTAtC7NfMCDjdgT0+C4W98A4TkV575f/z1yccwwib+Oxr/8ivf5g9OvUTwIw+BpCT9dJlotERmJoKoKXjSCoouKO+sIcoKwhCImiCQFJgq1GKSyEegv5Ahnw0gixqpf/UnnwyOWUn6M2VisQXS09EN5fj8Z37O919/9B7H73/lL/nDU19q60MYgsrwCg4d/EkFqVgc4ZtgPN+hj5CJ/+7W+fhY9OMXkKOTnjZx6BCYVe5xdNLTjZoX/fkMhTU4wOERhPSa+OYFRo/O9149yr998X/x5APXKI/W+G+Tx1CzGmoVMgclmtcgfzVO/JyGWlQYevwuz3zhHb7w8AV6d6dBwtBbJqUBE9MLv/bCKfb/gyuULvUgSxr4TWqh7nH0nPdsOMe/++L/bOaoQOaQxZG7kthYHzGDH7z2WJ2PP7/11Ko+lAWFgaPTPP3COV54+CLxcYtj8C1Jud/i+PLzp9n396927OPYQ1cd+XjxkfNb0o9fpJ465fjlz5+r4xh8e5njy8+f3rSeNnG8Jes4Dnx1C+dlh4npW56Xsg0OAOHkSTjfnp3ya999ih/eup/sZIzoVZX8bpPgXYXiTpORH5skH9Hw5KE4amL6TNSigtAFpk/iGy0w3jvPlXd3MfpDg5nHPfS+ZzD9UpXB3iyVbw9QCwkMP/SdmOKDf/YtZi7NN126bxTH3V+tMtS3eRz+XXnGEumu+Pin3znGa5P3teY4aZJ8uIFjQUHUln3sSqS5dmGU0VcNZo56SLxvMP0ly0f5lQH0oMAIQO9zd7jytT9bleMb332Cv5k8SOZWD9ErKvk9znzs7p3n8vmxbdmPTeFoWJdN4WjVj62YFxs9beRo2VM/9B5v39PtMC/g9E24qsLb/+ZxtL+K402pfO433yR6TSEwI/EUBMP/4jriSA7DC9GrCp60irKzhKLD4FtQSgXYE06hVAS3n9Go9pjMPqow2Jtl7uwghhcUXVLYq/NA/A65qn9TOYb6OuT438scsavC4shbHMoDWUxPa46FueCqPlJvL3LU7HH89A+O3vNx/DfeqvMx+M9vNPsYtjgG3rZ87IvMoVQFtz9jcSQ/tezDXPKxR+fBxFRbjjP/+gnU7yYsH79l+QhON/uIXREtfYyH5u/5qMQtH8N9Gcc+PrY9Hd54jsZ+DP/e6j6W+tHVeXHCMdyCo7rM0bKne9fu6dk/PIr6fxo4ZtcxL492MC843IDVgM78bxVIvFdAPZLlh//1KTKPVXnud85QHaqRKofQ3oyilUAPgbYg0Of91GImuk8QP69x8tY+fGlB/znryrvWq5N+YxDTK8nvASkE/t4S//fdh5AldWM4znWZ4/1ljvTRmsUxvMhxOoZabuYwvBA/p/H6ZGsOw7fIodjjSP1mkcSlIsrhHK9968k6H+lK0PLRkmPFuswL+s43+8gt+vAlbPj4R80+jv/u6SYftZBY04eQFkfq9aF7PhBsfD+2S0+7yLHUD/VIltf+7EnbHHX9WIPDTj/qOL7VIUc3evoPi/S+3zAvX1/HvCScc4DDDdgsaRRSQfj3ab5+6CSZIzqJsx7+6sMHEIpk4qcj1KJQGDN5/O9cwP94CjWvIP0myeMVCmNgXoxRGjQpfjmLVhBEL3mo9kg8B3IoFSiOSirTQZSshtRaH4+04oj/1AHH+MZxNPqoxlpzzJ6oUhgH40J3OIrzAfijeX7n8I/JHF72gVjhY1f7dVkYMln4e4scl5c51LLFUZ2x7+N3D/3ono/v3nhw2cfSuvzd9j4WvpxFyy/78N5n+SjsYuP7sV162kWOpX5sJIedftRxrNZTGxz3enqps54ucXytHUebeTHeW/+8gMMNWBgQuuZlIhXnT688zW8fewNTExiGwq7hFOwuogclUoEPMjsQQmJETKgJPD4dpQo7Xy8TvK1QLPjxZgXVHuu1S7cjRG6C0OFXnnwHrShAtjw2ackhVYccb6zgyHXIobfwsYJD7NlEH9e93JxL1PnQdZXR4fk6H5fSA6v6CE0u+6is5Jhw7uPlK8/wT37pZLOP0LIPVVndR6FhXRYmN7kf27GnXe6HYSiMDM1v3rx0qacrOSrx9XH8l6u/3BHHyOvrnxdweBsaIYNKQqK9F6GqSf7inWehH6qzQWYvRKiN1lBVSWBaYaa2A6libfF+k2reixqSTB73wb4CwlCohUHfv4BR9NA3lEW/0sfRZy/x41eO4qtZ99VtGMdzKzhCHXKEW3DsWOaoLnIE767tQ1mvj7hEuxSmqi77qCUDzF0M1/lI1vqd+7i6gkO37+PP3zm+IT5OfnsNjk9qT9fbj/fDTRxzmzkv3e5pWKLvK7XuqROOc93nePSzl9eeFxxuwGZNJX4J5o4aSI9p7eymBWb4FXwzGgM/MygMCaSm4M1A9rAOHkngIy/VHpPqUA1R0ZBllcgsVMtBKkdKLLzZR3VccubSPnyP5qmk/ZgB8+PFAWCIlhymZ5HjSG1VjvAG+tjxc4PigMXhS6/io6whKyqRJFQrQSqHSxTf6qM2ZnF4P1WgmvG15Ui8D8nHO/DxoZdq3KaPxzagH23WZat6WolvDw67Puz0o6s9nRVUSys4HPT0HsfS8UA7jgxkD63BUV6el+q45K3Le9fkAIcbsKhB5vkFju+9yrnkCMmJOHhNlKyGogsC05LcLo1yPxg+iekBT6yCUCSlnQoioCMyXpQamP1VsvcriHiV8M8C6EEwvZLxVyA3HoadoCitz05EDdKfK3F835Ut53DsI1rdMB/P7r3Gu3PDzN1MIH1GHUd+ZJmj5AEtWkVRzdYc9y1z1EKLHH8pyI2FKK7F8YLl4925nczcTNT5CN5t42OkgWPHJ2NdttO8LPVj4zhs9MNBT8ua/Z464tDX4JhZ37xIj715AaePIlcgESsyUUiQnIzTszNH7kYPwWnrKLmwC9QqGH4JJtQiEjPrQykrhO8oYGoUxw2MkIEQoJYUDOnFm5OUnyxipv0UBz0oNRCGQK5ydqJWoLen0DUOpaxgpjvj6LoPWnBU7fmYLPZYHCPZthx6WGLmvMscUqM41uzDk5eUnihiZvwUB7Q1fWgrfMzcShAfyZK9Hrc4JOTHHPigdT8WBuz56Oa6KGUFc97ysR360QlHXT+ccrTqR4c+nPS0Fumgp3Y4yhbHrUK8Nceog3mhBUfa3ryAww1Y0SXzuSBzl/sQPknuRg995wWGV1J6IYfQVWq3gxiJGkrGg29eQegK1bikkpAIHUSsihCABG9aACp6AAZ68syfCzP/gIkZkCBhtWdEFKO7HL55AaIDjo3wke7QRzZUx9H7rsD0WBzUVGpT7TmI1qyf1Ct8GAEYjOeZP2+PQzT4yF6PWz48ktLn19ePoXiOuXNhUg9uUT9QMfzboB+ZDjha+HDaj1bzshUcLXvqd9bTe/Nyp4HDKyk974xDmqLjeQGHd0EYfoG8FSI0qYAhCE8olBOCakwwlkgjTYHplwhFYvqscw/TA3KkROxICnNPCdVjYFZVpBRUek3KAybVGEze6kOYIAcrBG+pIEFUWuMZvtYclZ51cOzogKPBR+Rmgw+JxaG25pB7F7rnYzJY56MSX/aBtJ7gWfIhRbMPrZWPaIc+bjX46Fm/j4mJ/q3tRyfrssq8bHpPW/ho6sdaHJq56Ry2e9oBh3m7xbzE1p6XyOF6DqD1vAyszQEOr4ANH3hygmoMZNAgvwf8cwrVqCRZDKOnFp/4yHghpFMeNBCxKv09BeazIfSKiuYDJaehlhc/xGLOej3Fr6MVPWgeg9AdiVRVWOXKfbty5PY2cMwFABDzXmS4maNW1tB8Rj1H0rrFZyN9VAYMRLTRh4HIa2il9j5MzQZHT72PWqS9jx3xPKlMeMt8/KL1tDMOY9M5KgOtObrRU2+2nsOXsno6U+jOvKgeg9Dd9hzg9FHkxcPk8qBO31CW0K4cSDB3VIj6y2gFBalKUOW9bxoOlxFCos8G0Ga8mDN+grcVej6wfoWI3bTuq5NS4M1JfL4a+V0CPSQRfuMTwSE1GxyXFzkm1sEx0JpDbeUjUgao4whNru3DCDr3YQy099HIseTDn7I41IrF4cuan8h+bFeOluvSDQ4nPW3B0bWeDugkBrMER/MI03pDLeqvdGledPKja3Dg9D5gAYpu/ZtKhdk/MsvsUROz4uGj2/34ygJTU5AaSFPgnVPJ6zEKvRXUiiB0R2D4BKUBCQgqfZK7Ayqx6xIu+0k9bBKQAkWH2u4SprHKz4dN4PCbSkccyccNzLLXPoe3iz4Ui2PvSJK5FT68FYEsruBIKZ35MOxyyDofsmLTx1Qzx8KgxSEv+5l75OPfj+0wL636oRa74MN2P7ZLTyE9v8RhYFY8TEz1dcwRvbGCQ4q1OXB6BSwk1YeKiKpAmfMyme4hMx2hOhtESXrxZsGbVcCEQLyEHjLxZhQ8H/rx5AW1ECwMmXz1xBvon86j5a2D72DSoHRfBf+sgv5uD4YP9LwHUV3l2l1Iqg8X6jnuRus4PLkGjqxo4vj1E6eaOBYOWBzGhVhHHOkGjnY+9CAsDC/78ORa+Dhv08eKdZlKx+p8+DINPoKyMx9emz4qSp2PyoxNHyEoLfbDeCLXNR+d9rQr/WjkcNCPrs6Lk350Oi92+rFderqC4/b8GvuHbQ7T4kja5MDhBqwoklrBS/S6ilZQqN4ME7niQWoSI2JQ3GmiB6xfMUuzQcwdVarjZZDWOUktJjEjBidnDmBcCxOdMAlNKtx+TqDOeAlOS9Qy8EAeETBWPTtRFEkt7yNyYwXHVa2Ow/A3cIxVmjjemNnfxKHNLnPII/Y4OvVR7an3EbnVwkfFrg9vvY9rNnxg3RNZbfARuSWbfCiVzfFhRC0f+tXI+nwUGnx00NO1+tERR0NPuzEv9talg3448GG7H40+tqCnosFHbSJUx7EwbGI0rksLjpPTBzCuL3NMPYsjDnB6BQz4pzxkD+lEH0sihssUdhv0XNQYel1h7yO3CR1OE76pWq+c8aDMeTF81idbedOC0HUPM2eGMYKS1BFBLQqhCRW1LCi/lMH0gLgQQcx5aXP3Br47HnIHbXJkO+R4b22OVX2cXMOHaZNDs+njruUj8ugcDJcpjBv0vNfgY6LBhxeEBN8Sx5sWx/xhmjik5sxHz9FZez6Syz588+19VFb4UJJrc9juR4c95WKXe7oax5vr7+lSP+5xjLfgaOyHAw67/WjysQU9FbSfl92PTBE61IYjY3Ek3xrCCLTg+FX7Ppz9RQwpMFVJ+IbGXDLK0Letd7MrCXjy937KjXMjvLDrEiMv3kTNq3iyCpF9GYL3ZyjtMDE94Esv/tT2SMTYAuWxCsKEnmsmR/qnCd227p2LXRMIX+tH+KQUSGWZY/AVX3uOTGccwgaHqUrC15t9HPv9s+19DNjjCC/5uGrPR2ouwtArPmTIoBKv9zH8hYnVfWQaOHZVre97XXKof8axj9nZmC0f0QNpwgfTlAZMjDV8HF7hI2qHY2VPQ0bXeyrMLvd0NY7SBnC08NG2H3Y4Oplbpz1tXJcOe9puXj46t7M9h1Y/L+wqNXNMrc0BDt+EkxLijyY5nJjm9I8e4M4xGBxMMV1L8LO5MQJ7c7zy6jGUCrC3hD7rJzMVJfaBRlSHwi8toOhBBs+Wycz7yO0JooyUkSqYmuC97xwkljVAKPgzJt5Zexx3n4LBwQzTtQRnk+ObzvFA711OvfZgnQ87HMW/VaSih9pz5AzkHec+BgZTzOhxzibH8e/Jr85hQOHYAsoFy0d63kd+t8WB9TAWl75zP9GVHMnWv1PV+Xh1hQ+9wUcV2GNxpG/H6n28G2LwbRs+0vZ93DkGg0MWx3bp6ZZzDM01r0sHHIanBcca/ViNo21PGzhW62mnHE7nRVwMMni2QmbeS353oHlesgZyDQ5wuAH7PDqzySj5n+xAA6qjOlVdJZAokcyFqdVUwrcFpX4JUwEEEB4rkFXChG5qaB8EUapw5yk/nqL11w10Q6HcJynuMtHyCoU9EP1AUOpX6buSccyRyoeoVbW2HJ7L3eV4s1Mfl0Ib6sMfL6/qI7SrSFYJEZxYXJeaPY5Mv0rfB+nVOeZW+Ejo6IZKMLHAXD5EdY11uefjmB9PYZHDFE0cscuCdL9K31Xn/Ujmwpvej07m5d66bIN5cc7Rph8dcjj2scM5R9O87Fh9XtQq3H3St655AadPwqW8DP7AS2nYYGG8hn9aI5sPUil5KBe8GDkvhScWqI1V8O7O8+zT5xmM5vHMqwTvSrw568OWK/0m+UfK1tMlhkCPGMRGsyDBk1HIPlTFCMBUOtEZR97TlsOTtzjKO2xyZLrDceIz51py2PZhh2NMxz+jkSsE6jk+XaI2VsEzXuDZp88zsMgRWlqXXYs+Hq5gehc5wsscWnaRw78Gx/fqfaRzQcoLXkp536o+vKkWPj616ENXmnxkHt5m/eiUo828hO50MC/r6WlDP1rN7T0fLfrRkY8Zi6O84F2VY0ek0OSjHYfdng78jY15Gd3YeQGnRxBxnVKfgjBMlJyGHpIc+PoUU7++n/i1Gr5kiau/EcA/o1EL+jgl91GeCxAoCrL7JcKU+OYVKgmTnrd8mB5BUXiQu0pkb8dQxktIQ/Diwff4fvox6169LeIwdYUXD120OFb5PM+2HFdr+ObKdRwn5X7KcwGChQ592OGoCPRgg49UmWtfDeCbVdGDPk6xwse+Bo6zXouDFRxjZUxDdNdHwMfr7KOUDBK0sS6ZqSjqoo8vHbrIX6ePbmk/tktPHfdjDY6mfqzGQZt5seMjrzrraSsOb6c9NezNS1JFD2zcvIDTD+NJaYgTKbw/70UPSv7DS/+dbz70HJTTBL+U5fK1nYRuaHizkux9kidGb/KT2l7ktEZ1uAZCErriw5dSKI5Ihs7UyB4SJN6wHlFVywGityr86Imj7Dm1wEy09UoqKQ2Oz+N9J9Ga4+pOQh+ujyMyWeVHnz7K7p+051jLR/i6hidncTw1+hFv1PZhTmtUd1YBVueQoJUDRCad+/ijl/4HLz/0LLKcxv/FHFeuDRP6UMWbk2QPrOFj5yLHQUH8VAAhQS377fv43LKPP/7bf8EfP3RiVR9PjtzkjWqDjw98+OcsH8OnW/t49YnH1/SxWT1dbz/Wy7G0LuualyUOO/1Y57y04uiopyPLPV3msNtTFfFc/bz8pwefQ1YWOa4PE7rRpXk5XVqVA3D2Z+n9w6Ny/E//MaHvRUgfAk9R0H9OJ3zmBtln9nHnWYmaVwlNCfL7DZDgnVcI3rGePMntg1pcB02yb3yGLw5e4JtnToAq8d/yIlWJ4QdPVlAeNEj9yX8kf2W66QR70zhygvLABnHUILe/ez7G/vNvE/5BmPTBRY7zOuHTN8h+dj93Pmu25ri7yLHkQ5Xs2z3DFwYv8vKZ493zcfq6xbHk47Ygf2BjfXzs+7ER8/L9SHM/NtnHdulptzj2jM/y4tCFOg5Tk5i+tTnA6W1oXkn1owipT5mE9mfQA5KZr5ShN05uXAXNBEUSfn4a6TPwZBX0kKS4E6QCodsCpagiFlQ+Or+Tb545gTdWwR+tELojqfQbRA+m0EMSX1LFyHjscfg3iCNonyNyIN3M4WnmqIUXOdTu+qjdDJN6ZIWPXytDf4LcuLK6j2GLIzi1yFGyOF4+c/weR/CuxRG5f75zH32JOh+RFzbex3r6cc9HF/vRybzY5rDro1U/dm+uj+3S0zqOQOccE+8O8/Jpi8MXsTiqffY4wOEVsBAiCUzY/oL1Z0xK2e9yuBwuh8vxSeMAhxuwGzdu3LjpXhw/iuzGjRs3broTdwN248aNmy2KuwG7cePGzRbF3YDduHHjZovibsBu3Lhxs0VxN2A3bty42aK4G7AbN27cbFHcDdiNGzdutijuBuzGjRs3W5T/D479QOJvItftAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOEeqgmfMWd"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69tYF9PLpdHP"
      },
      "source": [
        "\n",
        "        "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnRqxuYDgiYo"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}