{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIST_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjWLS4AoCAmJqOghK2FyEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olivia-Feldman/NUGAN-DISTGAN/blob/Olivia/DIST_GAN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSigDUfUA0On"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch8zqg8bcneW"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb-qY4uZcprx"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqtgJep-ctzY"
      },
      "source": [
        "\n",
        "def visualize_results(gan,recon_images):\n",
        "\n",
        "      samples = (recon_images + 1) / 2\n",
        "      samples = samples.clamp(0,1)\n",
        "      samples = samples.reshape(recon_images.size(0),1,28,28)\n",
        "      samples = samples.cpu().data.numpy()\n",
        "      plt.figure(figsize=((1,5)))\n",
        "      fig,ax = plt.subplots(1,5)\n",
        "      for i in range(5):\n",
        "        s=ax[i].imshow(np.squeeze(samples[i,]))\n",
        "        s=ax[i].get_xaxis().set_visible(False)\n",
        "        s=ax[i].get_yaxis().set_visible(False)\n",
        "      s=plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XJXKI2sv2bx"
      },
      "source": [
        "def initialize_weights(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.ConvTranspose2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50yvRNhBdHuc"
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder,self).__init__()\n",
        "  \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(True), \n",
        "            nn.Linear(64, 12), \n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(12, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = self.encoder(x)\n",
        "        #x = x.view(x.size(0),-1)\n",
        "     \n",
        "        \n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3tMaTYAdSJm"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, input_size=28, base_size=128):\n",
        "        super(Generator, self).__init__()  \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.input_size = 28\n",
        "        self.base_size = base_size\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_dim, self.base_size)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, self.input_size* self.input_size )\n",
        "        initialize_weights(self)                    \n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "       # x = x.view(-1, self.input_size * self.input_size)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, input_size=28, base_size=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.input_size = input_size\n",
        "        self.base_size = base_size\n",
        "\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear( self.input_size* self.input_size,self.base_size)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, output_dim)\n",
        "\n",
        "        initialize_weights(self)\n",
        "\n",
        "     # forward method\n",
        "  def forward(self, x):\n",
        "       # x = x.view(-1, self.input_size * self.input_size)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "       # x = x.view(-1, self.input_size * self.input_size)\n",
        "      \n",
        "        return torch.sigmoid(self.fc4(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "732MQHr1dU-4"
      },
      "source": [
        "    def gradient_penalty( real_data, generated_data):\n",
        "        batch_size = real_data.size()[0]\n",
        "\n",
        "        # Calculate interpolation\n",
        "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
        "        alpha = alpha.expand_as(real_data)\n",
        "        if self.use_cuda:\n",
        "            alpha = alpha.cuda()\n",
        "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        if self.use_cuda:\n",
        "            interpolated = interpolated.cuda()\n",
        "\n",
        "        # Calculate probability of interpolated examples\n",
        "        prob_interpolated = self.D(interpolated)\n",
        "\n",
        "        # Calculate gradients of probabilities with respect to examples\n",
        "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda() if self.use_cuda else torch.ones(\n",
        "                               prob_interpolated.size()),\n",
        "                               create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
        "        # so flatten to easily take norm per example in batch\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
        "\n",
        "        # Derivatives of the gradient close to 0 can cause problems because of\n",
        "        # the square root, so manually calculate norm and add epsilon\n",
        "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "        # Return gradient penalty\n",
        "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBPFiScdbXy"
      },
      "source": [
        "import tensorflow as tf\n",
        "class GAN():\n",
        "    def __init__(self,params):\n",
        "        # parameters\n",
        "        self.epoch = params['max_epochs']\n",
        "        self.sample_num = 100\n",
        "        self.batch_size = params['base_size']\n",
        "        self.input_size = 28\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.base_size = params['base_size']\n",
        "\n",
        "     \n",
        "        self.lamda_p = 1.0     # regularization term of gradient penalty\n",
        "        self.lamda_r = 1.0    # autoencoders regularization term  \n",
        "       \n",
        "        \n",
        "        \n",
        "        # load dataset\n",
        "        self.data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=self.batch_size, \n",
        "                                               shuffle=True)\n",
        "        data = self.data_loader.__iter__().__next__()[0]\n",
        "\n",
        "        #print(data.shape[0])\n",
        "\n",
        "        # initialization of the generator and discriminator and autoencoder \n",
        "        self.A = autoencoder().cuda()\n",
        "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[0], input_size=self.input_size,base_size=self.base_size).cuda()\n",
        "        self.D = Discriminator(input_dim=data.shape[0], output_dim=1, input_size=self.input_size,base_size=self.base_size).cuda()\n",
        "      \n",
        "\n",
        "        self.A_optimizer =  optim.Adam(self.A.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']),eps=1e-09)\n",
        "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']),eps=1e-09)\n",
        "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']),eps=1e-09)\n",
        "        \n",
        "        # initialization of the loss function Hinge Embedding loss ( from paper )\n",
        "       \n",
        "        self.Hinge_Loss= nn.HingeEmbeddingLoss().cuda()\n",
        "     \n",
        "        \n",
        "        # Gettng a batch of noise to generate the fake data\n",
        "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim)).cuda()\n",
        "        \n",
        "# Fucntion to train the GAN, where you alternate between the training of the genenator and discriminator\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "       # Setting empty arrays for storing the losses\n",
        "\n",
        "        self.train_hist = {}\n",
        "        self.train_hist['D_loss'] = []\n",
        "        self.train_hist['G_loss'] = []\n",
        "\n",
        "        # Setting up the labels for real and fake images\n",
        "        self.y_real_, self.y_fake_ = torch.ones(self.batch_size,1).fill_(0.9).type(torch.float32).cuda(), torch.zeros(self.batch_size, 1).cuda()\n",
        "        \n",
        "        print('training start!!')\n",
        "\n",
        "        # Epoch loops\n",
        "\n",
        "        for epoch in range(self.epoch):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "\n",
        "            for iter, (x_, _) in enumerate(self.data_loader):\n",
        "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
        "                    break\n",
        "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
        "                x_, z_ = x_.cuda() ,z_.cuda()\n",
        "\n",
        "\n",
        "                #x_ = Variable(x_).cuda(device).type(Tensor)\n",
        "                x_= x_.view(x_.size(0), -1)\n",
        "                z_ =z_.view(z_.size(0),-1)\n",
        "                print(z_.shape)\n",
        "                #print(x_.shape)\n",
        "\n",
        "        \n",
        "\n",
        "                 #-------------Train Autoencoder & Generator to minimize reconstruction loss ------------# \n",
        "                  # auto-encoders and its regularization\n",
        "\n",
        "    \n",
        "                # reconstructed images \n",
        "                recon = gan.A(x_)\n",
        "                X_r  = gan.G(recon) # reconstructed iamges from generator \n",
        "                print(X_r.shape)\n",
        "                X_f = gan.G(z_) # fake images from generator \n",
        "                X_r, X_f = X_r.cuda(), X_f.cuda()\n",
        "\n",
        "\n",
        "                ## auto-encoders and Regularization \n",
        "\n",
        "                R_loss =torch.mean(gan.Hinge_Loss(x_,X_r))\n",
        "\n",
        "                f = torch.mean(X_r-X_f) #distance between reconstructed imgs and reconstructed fake imgs \n",
        "                g = torch.mean(recon-z_) # distance between reconstruced imgs and noise\n",
        "                R_reg = torch.square(f - g)\n",
        "                R_loss = R_loss + self.lamda_r * R_reg\n",
        "\n",
        "\n",
        "                #set up loss functions \n",
        "                gan.D_optimizer.zero_grad()\n",
        "\n",
        "                # compute gradient penalty and interpolation     \n",
        "                epsilon= torch.rand(x_.size())\n",
        "                epsilon.cuda()\n",
        "                interpolation = epsilon * x_ + (1-epsilon) * X_f\n",
        "                d_inter = gan.D(interpolation)\n",
        "              #-----------Train Discriminator minimize novel objective function  -------#\n",
        "\n",
        "                D_real = gan.D(x_)\n",
        "                D_recon = gan.D(recon_imgs)\n",
        "                D_fake = gan.D(fake_imgs)\n",
        "              \n",
        "\n",
        "                # Discriminator loss on data\n",
        "                d_loss_real = torch.mean(gan.Hinge_Loss(gan.y_real_, D_real))\n",
        "                d_loss_recon = torch.mean(gan.Hinge_Loss(x_, D_recon))\n",
        "                d_loss_fake = torch.mean(gan.Hinge_Loss(gan.y_fake_,D_fake))\n",
        "        \n",
        "\n",
        "                D_loss = (d_loss_real + d_loss_recon)*0.5 + d_loss_fake\n",
        "                D_loss = D_loss + self.lamda_p * gp\n",
        "              \n",
        "                \n",
        "      \n",
        "\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                D_loss.backward()\n",
        "                # 4. Make a step of D_optimizer\n",
        "                gan.D_optimizer.step()\n",
        "\n",
        "                # 5. Set the current loss in self.train_hist['D_loss]\n",
        "                gan.train_hist['D_loss'].append(D_loss.item())\n",
        "                \n",
        "                # update G network using \n",
        "               \n",
        "                gan.G_optimizer.zero_grad()\n",
        "\n",
        "                 #---------------Train Generator to minimize discriminator score --------#\n",
        "                \n",
        "                recon_fake = gan.G(_ze)\n",
        "                D_fake = gan.D(recon_fake)\n",
        "                \n",
        "                G_loss = tf.abs(torch.mean(D_real) - torch.mean(D_fake))\n",
        "\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                G_loss.backward()\n",
        "                # 4. Make a step of G_optimizer\n",
        "                gan.G_optimizer.step()\n",
        "                # 5. Set the current loss in self.train_hist['G_loss]    \n",
        "                gan.train_hist['G_loss'].append(G_loss.item())\n",
        "\n",
        "                # Print iterations and losses\n",
        "\n",
        "                if ((iter + 1) % 50) == 0:\n",
        "                  print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
        "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
        "    \n",
        "                  \n",
        "            # Visualize results\n",
        "            with torch.no_grad():\n",
        "                visualize_results(self,recon_images=recon_real)\n",
        "        #plt.figure(figsize=(16,8))\n",
        "       # s=plt.plot(gan.train_hist['D_loss'],c='b')\n",
        "        #s=plt.plot(gan.train_hist['G_loss'],c='r')\n",
        "        #s = plt.ylim((0,1))\n",
        "        #s = plt.grid()\n",
        "       # s=plt.legend(('Discriminator loss','Generator loss'))\n",
        "\n",
        "        print(\"Training finished!\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E24iw-EczhD1"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEXhP73SyT2m"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW-BInEbtLlE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "4f8c4fa7-bc17-4c6d-b3f1-15da4a40c6a8"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "params = {'beta1': 0.05, 'beta2': 0.999,'lr_g':0.0002,'lr_d':0.0002,'max_epochs':30}\n",
        "params['z_dim'] =2\n",
        "params['base_size'] = 128\n",
        "\n",
        "gan = GAN(params)\n",
        "\n",
        "\n",
        "\n",
        "gan.train()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start!!\n",
            "torch.Size([128, 2])\n",
            "torch.Size([128, 784])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-06d05c03ff11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0661075ea14b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mepsilon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0md_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m               \u001b[0;31m#-----------Train Discriminator minimize novel objective function  -------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOEeqgmfMWd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69tYF9PLpdHP"
      },
      "source": [
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnRqxuYDgiYo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}