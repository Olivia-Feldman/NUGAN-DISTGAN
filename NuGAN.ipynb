{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NuGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuRtj9sSl3mZuF8XBqRg11",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olivia-Feldman/NUGAN-DISTGAN/blob/Andrew/NuGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8isMKhlx0xr"
      },
      "source": [
        "# Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk03kg9rxO4W"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg5lYCj_yCje"
      },
      "source": [
        "# Download the MNIST dataset\n",
        "\n",
        "[MNIST PyTorch Docs](https://pytorch.org/vision/stable/datasets.html#mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ43QkgQyIkV"
      },
      "source": [
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_BZppFLyWBC"
      },
      "source": [
        "# Showing shape of train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPO3ZvOPybpd",
        "outputId": "51d8dace-8acb-4ac8-9589-29f9e68c6f76"
      },
      "source": [
        "print(\"Train Shape:\", train_dataset.data.shape, \"Test Shape:\", test_dataset.data.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape: torch.Size([60000, 28, 28]) Test Shape: torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ_WamIMwfXI"
      },
      "source": [
        "# Define a function to initialize the weights of the network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qbzeYmwdCE"
      },
      "source": [
        "def initialize_weights(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.ConvTranspose2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsrvh6Ruw3L9"
      },
      "source": [
        "def visualize_results(gan):\n",
        "      sample_z_ = torch.rand((10, gan.z_dim)).cuda()\n",
        "      samples = gan.G(sample_z_)\n",
        "      samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
        "      samples = (samples + 1) / 2\n",
        "      plt.figure(figsize=((1,10)))\n",
        "      fig,ax = plt.subplots(1,10)\n",
        "      for i in range(10):\n",
        "          s=ax[i].imshow(np.squeeze(samples[i,]))\n",
        "          s=ax[i].get_xaxis().set_visible(False)\n",
        "          s=ax[i].get_yaxis().set_visible(False)\n",
        "      s=plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DEvIxxCsmBS"
      },
      "source": [
        "from scipy.sparse import linalg\n",
        "\n",
        "def get_eigenvectors(model, loss, dataloader):\n",
        "    # Adds up the number of weights in the model\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    hv = HessianVector(model=model, \n",
        "                       dataloader=dataloader, \n",
        "                       loss=loss)\n",
        "    \n",
        "    A = linalg.LinearOperator((num_params,num_params), matvec=lambda v: hv.calculate(torch.tensor(v).float()))\n",
        "\n",
        "    print(\"Got eigenvectors\", A.shape)\n",
        "    # Gets 'k' with the Largest (algebraic) eigenvalues\n",
        "    vals, vecs = linalg.eigsh(A, k=2, mode='LA')\n",
        "\n",
        "    print(\"Eigenvalues are {:.2f} and {:.2f}\".format(vals[0],vals[1]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAJHsVddd4_F"
      },
      "source": [
        "# Calculates the Eigenvalues of the Discriminator\n",
        "\n",
        "\n",
        "[linalg.eigsh](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-rhoEvbQYAJ"
      },
      "source": [
        "def get_d_eigenvectors(D, G, loss, dataloader):\n",
        "\n",
        "    num_params = sum(p.numel() for p in D.parameters())\n",
        "\n",
        "    hv = HessianVector(model=D, \n",
        "                       dataloader=dataloader, \n",
        "                       loss=loss)\n",
        "    \n",
        "    A = linalg.LinearOperator((num_params,num_params), matvec=lambda v: hv.calculate(torch.tensor(v).float()))\n",
        "\n",
        "    print(\"Got eigenvectors\", A.shape)\n",
        "    # Gets 'k' with the Largest (algebraic) eigenvalues\n",
        "    vals, vecs = linalg.eigsh(A, k=2, mode='LA')\n",
        "\n",
        "    print(\"Eigenvalues are {:.2f} and {:.2f}\".format(vals[0],vals[1]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tptp9JXltklf"
      },
      "source": [
        "# Hessian Vector Class\n",
        "\n",
        "\n",
        "\n",
        "*   scipy.sparse.linalg.LinearOperator\n",
        "  *   [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S6BZlQHtkJP"
      },
      "source": [
        "class HessianVector():\n",
        "\n",
        "    def __init__(self, model, dataloader, loss, percentage=0.2):\n",
        "        self.size = int(sum(param.numel() for param in model.parameters()))\n",
        "        self.grad_vec = torch.zeros(self.size)\n",
        "        self.percentage = percentage\n",
        "        self.dataloader = dataloader\n",
        "        self.model = model\n",
        "        self.loss = loss\n",
        "\n",
        "\n",
        "    def calculate(self, vector):\n",
        "        full_hessian = None\n",
        "        # Place vector on the GPU\n",
        "        vector = vector.cuda()\n",
        "\n",
        "        grad_vec = None\n",
        "\n",
        "        batch_grad = self.prepare_grad()\n",
        "\n",
        "        #print(\"Batch Shape:\", batch_grad.shape)\n",
        "\n",
        "        self.zero_grad()\n",
        "        # Take the second gradient\n",
        "        grad_grad = torch.autograd.grad(batch_grad, self.model.parameters(), grad_outputs=vector, only_inputs=True, retain_graph=True)\n",
        "        # Concatentate the results over the different components of the network\n",
        "        hessian_vec_prod = torch.cat([g.contiguous().view(-1) for g in grad_grad])\n",
        "        # Adds/Sets Full Hessian\n",
        "        if full_hessian is not None:\n",
        "            full_hessian += hessian_vec_prod\n",
        "        else:\n",
        "            full_hessian = hessian_vec_prod\n",
        "\n",
        "        # Hessian must be returned on the CPU\n",
        "        return full_hessian.cpu()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Zeros out the gradient info for each parameters and vectorize\n",
        "        \"\"\"\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            if p.grad is not None:\n",
        "                p.grad.data.zero_()\n",
        "\n",
        "\n",
        "    def prepare_grad(self):\n",
        "\n",
        "\n",
        "        grad_vec = None\n",
        "\n",
        "        grad_dict = torch.autograd.grad(self.loss, self.model.parameters(), create_graph=True, retain_graph=True)\n",
        "        grad_vec = torch.cat([g.contiguous().view(-1) for g in grad_dict])\n",
        "        self.grad_vec = grad_vec\n",
        "        return self.grad_vec\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_jzg_VB0VHj"
      },
      "source": [
        "# Generator and Discriminator class definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAhXVtAo0USV"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=100, output_dim=1, input_size=32, base_size=64):\n",
        "        super(Generator, self).__init__()  \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.input_size = 28\n",
        "        self.base_size = base_size    \n",
        "\n",
        "\n",
        "        # self.fc1 = nn.Linear(self.input_dim, self.base_size)\n",
        "        # self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        # self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        # self.fc4 = nn.Linear(self.fc3.out_features, self.output_dim * self.input_size * self.input_size)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 2*self.base_size * (self.input_size // 4) * (self.input_size // 4)),\n",
        "            nn.BatchNorm1d(2*self.base_size * (self.input_size // 4) * (self.input_size // 4)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*self.base_size, self.base_size, 4, 2, 1),\n",
        "            nn.BatchNorm2d(self.base_size),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(self.base_size, self.output_dim, 4, 2, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        initialize_weights(self)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "\n",
        "        # x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        # x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        # x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        # x = torch.tanh(self.fc4(x))\n",
        "        # x = x.view(-1, self.output_dim, self.input_size, self.input_size)\n",
        "        # return x\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, 2*self.base_size, (self.input_size // 4), (self.input_size // 4))\n",
        "        x = self.deconv(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim=1, output_dim=1, input_size=32, base_size=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.input_size = input_size\n",
        "        self.base_size = base_size\n",
        "\n",
        "        # self.fc1 = nn.Linear(self.input_size * self.input_size, self.base_size)\n",
        "        # self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        # self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        # self.fc4 = nn.Linear(self.fc3.out_features, self.output_dim)\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_dim, self.base_size, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(self.base_size, 2*self.base_size, 4, 2, 1),\n",
        "            nn.BatchNorm2d(2*self.base_size),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2*self.base_size * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, self.output_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        initialize_weights(self)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x = x.view(-1, self.input_size * self.input_size)\n",
        "        # x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        # x = F.dropout(x, 0.3)\n",
        "        # x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        # x = F.dropout(x, 0.3)\n",
        "        # x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        # x = F.dropout(x, 0.3)\n",
        "        # return torch.sigmoid(self.fc4(x))\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = x.view(-1, 2*self.base_size * (self.input_size // 4) * (self.input_size // 4))\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PgTyYPe0qhu"
      },
      "source": [
        "# Construction of GAN model\n",
        "\n",
        "[Modifying gradients in PyTorch](https://discuss.pytorch.org/t/how-to-modify-the-gradient-manually/7483)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUZu_Ztd0uD3"
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self,params):\n",
        "        # parameters\n",
        "        self.epoch = params['max_epochs']\n",
        "        self.sample_num = 100\n",
        "        self.batch_size = 300\n",
        "        self.input_size = 28\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.base_size = params['base_size']\n",
        "\n",
        "        # load dataset\n",
        "        self.data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=self.batch_size, \n",
        "                                               shuffle=True)\n",
        "        self.test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                               batch_size=self.batch_size, \n",
        "                                               shuffle=True)\n",
        "        data = self.data_loader.__iter__().__next__()[0]\n",
        "\n",
        "        #mnist_dim = data.shape[1] * data.shape[2]\n",
        "\n",
        "        #print(data.shape)\n",
        "        #self.data_width = data.shape[2]\n",
        "        #self.data_height = data.shape[3]\n",
        "\n",
        "        # initialization of the generator and discriminator\n",
        "        self.G = Generator(\n",
        "            input_dim=self.z_dim,       # 16\n",
        "            output_dim=data.shape[1],   # 1\n",
        "            input_size=self.input_size, # 28\n",
        "            base_size=self.base_size    # 64\n",
        "            ).cuda()\n",
        "\n",
        "        self.D = Discriminator(\n",
        "            input_dim=data.shape[1],    \n",
        "            output_dim=1,\n",
        "            input_size=self.input_size, \n",
        "            base_size=self.base_size\n",
        "            ).cuda()\n",
        "\n",
        "        #print(\"Encoder\", data.shape[1], self.input_size, self.base_size)\n",
        "        #self.E = encoder(input_dim=data.shape[1], output_dim=self.z_dim, input_size=self.input_size, base_size=self.base_size).cuda()\n",
        "        #self.G_optimizer = optim.SGD(self.G.parameters(), lr=params['lr_g'])\n",
        "        #self.D_optimizer = optim.SGD(self.D.parameters(), lr=params['lr_d'])\n",
        "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']))\n",
        "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=params['lr_d'], betas=(params['beta1'], params['beta2']))\n",
        "        #self.E_optimizer = optim.Adam(self.E.parameters(), lr=1e-2, weight_decay=1e-4)\n",
        "        \n",
        "        # initialization of the loss function\n",
        "\n",
        "        self.BCE_loss = nn.BCELoss().cuda()\n",
        "        #self.smooth_loss = nn.SmoothL1Loss().cuda()\n",
        "        \n",
        "        # Gettng a batch of noise to generate the fake data\n",
        "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim)).cuda()\n",
        "\n",
        "        # Function to train the GAN, where you alternate between the training of the genenator and discriminator\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "       # Setting empty arrays for storing the losses\n",
        "\n",
        "        Ninner = 1\n",
        "        self.train_hist = {}\n",
        "        self.train_hist['D_loss'] = []\n",
        "        self.train_hist['G_loss'] = []\n",
        "\n",
        "        # Setting up the labels for real and fake images\n",
        "        #self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1).cuda(), torch.zeros(self.batch_size, 1).cuda()\n",
        "        self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1).fill_(0.9).type(torch.float32).cuda(), torch.zeros(self.batch_size, 1).fill_(0.1).type(torch.float32).cuda()\n",
        "        # print(self.y_real_.shape)\n",
        "        self.y_real_ = self.y_real_ + (torch.randn(self.y_real_.shape)*0.03).cuda()\n",
        "        self.y_fake_ = self.y_fake_ + (torch.randn(self.y_fake_.shape)*0.03).cuda()\n",
        "\n",
        "        print('training start!!')\n",
        "\n",
        "        randn_var = 0.05\n",
        "        randn_mean = 0.00\n",
        "\n",
        "        # Epoch loops\n",
        "\n",
        "        for epoch in range(self.epoch):\n",
        "            epoch_start_time = time.time()\n",
        "            print(\"Epoch:\", epoch+1)\n",
        "\n",
        "            for iter, (x_, _) in enumerate(self.data_loader):\n",
        "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
        "                    break\n",
        "\n",
        "                # Generate random noise to push through the generator   \n",
        "\n",
        "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
        "                x_, z_ = x_.cuda(), z_.cuda()\n",
        "\n",
        "                # YOUR CODE HERE\n",
        "                #--------------------\n",
        "\n",
        "                # update D network using \n",
        "                # 1. Set optimizer gradient to zero\n",
        "                self.D_optimizer.zero_grad()\n",
        "                # 2. Set discriminator losses on real and fake data\n",
        "                x_ = x_ + torch.normal(mean=randn_mean, std=randn_var, size=x_.shape).cuda()\n",
        "                #x_ = x_ + (torch.randn(x_.shape)*randn_var + randn_mean).cuda() # Adding random noise to input images\n",
        "                D_real = self.D(x_)\n",
        "                D_real_loss = self.BCE_loss(D_real, self.y_real_)\n",
        "\n",
        "                z_ = z_ + torch.normal(mean=randn_mean, std=randn_var, size=z_.shape).cuda()\n",
        "\n",
        "                G_ = self.G(z_)\n",
        "                D_fake = self.D(G_)\n",
        "                D_fake_loss = self.BCE_loss(D_fake, self.y_fake_)\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                D_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "                if iter == 0:\n",
        "                    get_eigenvectors(self.D, D_loss, self.data_loader)\n",
        "\n",
        "                D_loss.backward()\n",
        "\n",
        "                # if iter == 0:\n",
        "                #     print(\"D eigen vectors:\")\n",
        "                #     get_eigenvectors(self.D, self.data_loader, self.BCE_loss)\n",
        "\n",
        "\n",
        "                # 4. Make a step of D_optimizer\n",
        "                self.D_optimizer.step()\n",
        "                # 5. Set the current loss in self.train_hist['D_loss]\n",
        "                self.train_hist['D_loss'].append(D_loss.item())\n",
        "                \n",
        "                # update G network using \n",
        "                # 1. Set optimizer gradient to zero\n",
        "                self.G_optimizer.zero_grad()\n",
        "                # 2. Set generator losses on fake data\n",
        "                G_ = self.G(z_)\n",
        "                D_fake_ = self.D(G_)\n",
        "                G_loss = self.BCE_loss(D_fake_, self.y_real_)\n",
        "                if iter == 0:\n",
        "                    get_eigenvectors(self.G, G_loss, self.data_loader)\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                G_loss.backward()\n",
        "\n",
        "                # if iter == 0:\n",
        "                #     print(\"G eigen vectors:\")\n",
        "                #     get_eigenvectors(self.G, self.data_loader, self.BCE_loss)\n",
        "\n",
        "                # 4. Make a step of G_optimizer\n",
        "                self.G_optimizer.step()\n",
        "                # 5. Set the current loss in self.train_hist['G_loss]    \n",
        "                self.train_hist['G_loss'].append(G_loss.item())\n",
        "\n",
        "                # Print iterations and losses\n",
        "                \n",
        "                if ((iter + 1) % 50) == 0:\n",
        "                  print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
        "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
        "                \n",
        "            # Visualize results\n",
        "            if ((epoch + 1) % 10) == 0:\n",
        "                with torch.no_grad():\n",
        "                    visualize_results(self)\n",
        "                #visualize_gan_optim_loss(self)\n",
        "\n",
        "        print(\"Training finished!\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlpS9IeX5Kac"
      },
      "source": [
        "# Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xKeKUuUR5JuM",
        "outputId": "3daec295-d952-4d54-9db3-957eea201946"
      },
      "source": [
        "params = {'beta1': 0.5, 'beta2': 0.999,'lr_g':0.0002,'lr_d':0.0002,'max_epochs':10}\n",
        "params['z_dim'] = 16\n",
        "params['base_size'] = 64\n",
        "\n",
        "gan = GAN(params)\n",
        "gan.train()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start!!\n",
            "Epoch: 1\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are -27.85 and 50.61\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are -32.57 and -29.65\n",
            "Epoch: [ 1] [  50/ 200] D_loss: 1.31256151, G_loss: 0.81882280\n",
            "Epoch: [ 1] [ 100/ 200] D_loss: 1.20647335, G_loss: 0.86485463\n",
            "Epoch: [ 1] [ 150/ 200] D_loss: 1.11277127, G_loss: 0.94226909\n",
            "Epoch: [ 1] [ 200/ 200] D_loss: 1.07181764, G_loss: 0.99011821\n",
            "Epoch: 2\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 36.83 and 69.00\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are -27.76 and 9.26\n",
            "Epoch: [ 2] [  50/ 200] D_loss: 1.13000238, G_loss: 0.98462600\n",
            "Epoch: [ 2] [ 100/ 200] D_loss: 1.10195065, G_loss: 0.97576195\n",
            "Epoch: [ 2] [ 150/ 200] D_loss: 1.04023528, G_loss: 1.05375302\n",
            "Epoch: [ 2] [ 200/ 200] D_loss: 1.22572851, G_loss: 1.02526772\n",
            "Epoch: 3\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are -210.47 and 437.72\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 7.78 and 9.14\n",
            "Epoch: [ 3] [  50/ 200] D_loss: 1.09726858, G_loss: 1.04378307\n",
            "Epoch: [ 3] [ 100/ 200] D_loss: 1.18912458, G_loss: 0.90133995\n",
            "Epoch: [ 3] [ 150/ 200] D_loss: 1.17561197, G_loss: 0.90862793\n",
            "Epoch: [ 3] [ 200/ 200] D_loss: 1.15598047, G_loss: 0.91196191\n",
            "Epoch: 4\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 405.37 and 1607.85\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 22.86 and 28.88\n",
            "Epoch: [ 4] [  50/ 200] D_loss: 1.21628666, G_loss: 0.98144084\n",
            "Epoch: [ 4] [ 100/ 200] D_loss: 1.20514607, G_loss: 0.83639979\n",
            "Epoch: [ 4] [ 150/ 200] D_loss: 1.24705946, G_loss: 0.85325205\n",
            "Epoch: [ 4] [ 200/ 200] D_loss: 1.21482110, G_loss: 0.85540408\n",
            "Epoch: 5\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 276.56 and 2089.77\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 35.17 and 53.33\n",
            "Epoch: [ 5] [  50/ 200] D_loss: 1.20587635, G_loss: 0.87205833\n",
            "Epoch: [ 5] [ 100/ 200] D_loss: 1.21941376, G_loss: 0.89241844\n",
            "Epoch: [ 5] [ 150/ 200] D_loss: 1.25897908, G_loss: 0.94980675\n",
            "Epoch: [ 5] [ 200/ 200] D_loss: 1.17750573, G_loss: 0.89013392\n",
            "Epoch: 6\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 569.06 and 1544.51\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 44.67 and 50.03\n",
            "Epoch: [ 6] [  50/ 200] D_loss: 1.30142558, G_loss: 0.93629265\n",
            "Epoch: [ 6] [ 100/ 200] D_loss: 1.27752519, G_loss: 0.86631626\n",
            "Epoch: [ 6] [ 150/ 200] D_loss: 1.23484921, G_loss: 0.88002229\n",
            "Epoch: [ 6] [ 200/ 200] D_loss: 1.27144361, G_loss: 0.81999457\n",
            "Epoch: 7\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 442.94 and 1391.43\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 42.36 and 46.14\n",
            "Epoch: [ 7] [  50/ 200] D_loss: 1.28901982, G_loss: 0.80051059\n",
            "Epoch: [ 7] [ 100/ 200] D_loss: 1.20534086, G_loss: 0.92586589\n",
            "Epoch: [ 7] [ 150/ 200] D_loss: 1.23870015, G_loss: 0.86246401\n",
            "Epoch: [ 7] [ 200/ 200] D_loss: 1.20491874, G_loss: 0.86856365\n",
            "Epoch: 8\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 742.92 and 2370.29\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 71.56 and 128.34\n",
            "Epoch: [ 8] [  50/ 200] D_loss: 1.16453624, G_loss: 1.00120389\n",
            "Epoch: [ 8] [ 100/ 200] D_loss: 1.19411278, G_loss: 0.89307398\n",
            "Epoch: [ 8] [ 150/ 200] D_loss: 1.22558570, G_loss: 0.89057398\n",
            "Epoch: [ 8] [ 200/ 200] D_loss: 1.20677495, G_loss: 0.96977061\n",
            "Epoch: 9\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 683.27 and 2796.00\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 86.54 and 96.74\n",
            "Epoch: [ 9] [  50/ 200] D_loss: 1.17658794, G_loss: 0.95861912\n",
            "Epoch: [ 9] [ 100/ 200] D_loss: 1.20294499, G_loss: 0.81541413\n",
            "Epoch: [ 9] [ 150/ 200] D_loss: 1.28209889, G_loss: 0.81366372\n",
            "Epoch: [ 9] [ 200/ 200] D_loss: 1.15683722, G_loss: 0.90195376\n",
            "Epoch: 10\n",
            "Got eigenvectors (6559169, 6559169)\n",
            "Eigenvalues are 808.23 and 2127.55\n",
            "Got eigenvectors (6593089, 6593089)\n",
            "Eigenvalues are 110.02 and 126.08\n",
            "Epoch: [10] [  50/ 200] D_loss: 1.24315381, G_loss: 0.79345661\n",
            "Epoch: [10] [ 100/ 200] D_loss: 1.18284857, G_loss: 0.93393290\n",
            "Epoch: [10] [ 150/ 200] D_loss: 1.22256422, G_loss: 1.01251733\n",
            "Epoch: [10] [ 200/ 200] D_loss: 1.14829719, G_loss: 0.79712015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 72x720 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}