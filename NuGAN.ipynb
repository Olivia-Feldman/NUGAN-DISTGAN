{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NuGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnVJa7v3aD96okQyapn0LB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olivia-Feldman/NUGAN-DISTGAN/blob/Andrew/NuGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8isMKhlx0xr"
      },
      "source": [
        "# Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk03kg9rxO4W"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg5lYCj_yCje"
      },
      "source": [
        "# Download the MNIST dataset\n",
        "\n",
        "[MNIST PyTorch Docs](https://pytorch.org/vision/stable/datasets.html#mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ43QkgQyIkV"
      },
      "source": [
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_BZppFLyWBC"
      },
      "source": [
        "# Showing shape of train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPO3ZvOPybpd",
        "outputId": "b41eb251-25ad-4bc2-8567-b52f74ad06fe"
      },
      "source": [
        "print(\"Train Shape:\", train_dataset.data.shape, \"Test Shape:\", test_dataset.data.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape: torch.Size([60000, 28, 28]) Test Shape: torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_jzg_VB0VHj"
      },
      "source": [
        "# Generator and Discriminator class definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAhXVtAo0USV"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 28)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PgTyYPe0qhu"
      },
      "source": [
        "# Construction of GAN model\n",
        "\n",
        "[Modifying gradients in PyTorch](https://discuss.pytorch.org/t/how-to-modify-the-gradient-manually/7483)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUZu_Ztd0uD3"
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self,params):\n",
        "        # parameters\n",
        "        self.epoch = params['max_epochs']\n",
        "        self.sample_num = 100\n",
        "        self.batch_size = 300\n",
        "        self.input_size = 28\n",
        "        self.z_dim = params['z_dim']\n",
        "        self.base_size = params['base_size']\n",
        "\n",
        "        # load dataset\n",
        "        self.data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=self.batch_size, \n",
        "                                               shuffle=True)\n",
        "        self.test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                               batch_size=self.batch_size, \n",
        "                                               shuffle=True)\n",
        "        data = self.data_loader.__iter__().__next__()[0]\n",
        "\n",
        "        mnist_dim = data.shape[1] * data.shape[2]\n",
        "\n",
        "        print(data.shape)\n",
        "        self.data_width = data.shape[2]\n",
        "        self.data_height = data.shape[3]\n",
        "\n",
        "        # initialization of the generator and discriminator\n",
        "        #self.G = generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size,base_size=self.base_size).cuda()\n",
        "        self.G = Generator(g_input_dim = self.z_dim, g_output_dim = mnist_dim).cuda()\n",
        "        #self.D = discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size,base_size=self.base_size).cuda()\n",
        "        self.D = Discriminator(mnist_dim).cuda()\n",
        "        #print(\"Encoder\", data.shape[1], self.input_size, self.base_size)\n",
        "        #self.E = encoder(input_dim=data.shape[1], output_dim=self.z_dim, input_size=self.input_size, base_size=self.base_size).cuda()\n",
        "        #self.G_optimizer = optim.SGD(self.G.parameters(), lr=params['lr_g'])\n",
        "        #self.D_optimizer = optim.SGD(self.D.parameters(), lr=params['lr_d'])\n",
        "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=params['lr_g'], betas=(params['beta1'], params['beta2']))\n",
        "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=params['lr_d'], betas=(params['beta1'], params['beta2']))\n",
        "        #self.E_optimizer = optim.Adam(self.E.parameters(), lr=1e-2, weight_decay=1e-4)\n",
        "        \n",
        "        # initialization of the loss function\n",
        "\n",
        "        self.BCE_loss = nn.BCELoss().cuda()\n",
        "        #self.smooth_loss = nn.SmoothL1Loss().cuda()\n",
        "        \n",
        "        # Gettng a batch of noise to generate the fake data\n",
        "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim)).cuda()\n",
        "\n",
        "        # Function to train the GAN, where you alternate between the training of the genenator and discriminator\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "       # Setting empty arrays for storing the losses\n",
        "\n",
        "        Ninner = 1\n",
        "        self.train_hist = {}\n",
        "        self.train_hist['D_loss'] = []\n",
        "        self.train_hist['G_loss'] = []\n",
        "\n",
        "        # Setting up the labels for real and fake images\n",
        "        #self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1).cuda(), torch.zeros(self.batch_size, 1).cuda()\n",
        "        self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1, self.data_width, self.data_height).fill_(0.9).type(torch.float32).cuda(), torch.zeros(self.batch_size, 1, self.data_width, self.data_height).fill_(0.1).type(torch.float32).cuda()\n",
        "        print(self.y_real_.shape)\n",
        "        self.y_real_ = self.y_real_ + (torch.randn(self.y_real_.shape)*0.03).cuda()\n",
        "        self.y_fake_ = self.y_fake_ + (torch.randn(self.y_fake_.shape)*0.03).cuda()\n",
        "\n",
        "        print('training start!!')\n",
        "\n",
        "        randn_var = 0.05\n",
        "        randn_mean = 0.00\n",
        "\n",
        "        # Epoch loops\n",
        "\n",
        "        for epoch in range(self.epoch):\n",
        "            epoch_start_time = time.time()\n",
        "            print(\"Epoch:\", epoch+1)\n",
        "\n",
        "            for iter, (x_, _) in enumerate(self.data_loader):\n",
        "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
        "                    break\n",
        "\n",
        "                # Generate random noise to push through the generator   \n",
        "\n",
        "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
        "                x_, z_ = x_.cuda(), z_.cuda()\n",
        "\n",
        "                # YOUR CODE HERE\n",
        "                #--------------------\n",
        "\n",
        "                # update D network using \n",
        "                # 1. Set optimizer gradient to zero\n",
        "                self.D_optimizer.zero_grad()\n",
        "                # 2. Set discriminator losses on real and fake data\n",
        "                x_ = x_ + torch.normal(mean=randn_mean, std=randn_var, size=x_.shape).cuda()\n",
        "                #x_ = x_ + (torch.randn(x_.shape)*randn_var + randn_mean).cuda() # Adding random noise to input images\n",
        "                print(\"x_\", x_.shape, \"self.y_real_\", self.y_real_.shape)\n",
        "                D_real = self.D(x_)\n",
        "                print(\"D_real\", D_real.shape)\n",
        "                D_real_loss = self.BCE_loss(D_real, self.y_real_)\n",
        "\n",
        "                z_ = z_ + torch.normal(mean=randn_mean, std=randn_var, size=z_.shape).cuda()\n",
        "                G_ = self.G(z_)\n",
        "                D_fake = self.D(G_)\n",
        "                D_fake_loss = self.BCE_loss(D_fake, self.y_fake_)\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                D_loss = D_real_loss + D_fake_loss\n",
        "                D_loss.backward()\n",
        "\n",
        "                print(len(D.parameters()))\n",
        "\n",
        "                # 4. Make a step of D_optimizer\n",
        "                self.D_optimizer.step()\n",
        "                # 5. Set the current loss in self.train_hist['D_loss]\n",
        "                self.train_hist['D_loss'].append(D_loss.item())\n",
        "                \n",
        "                # update G network using \n",
        "                # 1. Set optimizer gradient to zero\n",
        "                self.G_optimizer.zero_grad()\n",
        "                # 2. Set generator losses on fake data\n",
        "                G_ = self.G(z_)\n",
        "                D_fake_ = self.D(G_)\n",
        "                G_loss = self.BCE_loss(D_fake_, self.y_real_)\n",
        "                # 3. Do back propagation to compute gradients\n",
        "                G_loss.backward()\n",
        "                # 4. Make a step of G_optimizer\n",
        "                self.G_optimizer.step()\n",
        "                # 5. Set the current loss in self.train_hist['G_loss]    \n",
        "                self.train_hist['G_loss'].append(G_loss.item())\n",
        "\n",
        "                # Print iterations and losses\n",
        "                \n",
        "                if ((iter + 1) % 50) == 0 and ((epoch + 1) % 10) == 0:\n",
        "                  print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
        "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
        "                \n",
        "            # Visualize results\n",
        "            if ((epoch + 1) % 10) == 0:\n",
        "                with torch.no_grad():\n",
        "                    visualize_results(self)\n",
        "                    #visualize_gan_optim_loss(self)\n",
        "\n",
        "        print(\"Training finished!\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlpS9IeX5Kac"
      },
      "source": [
        "# Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "xKeKUuUR5JuM",
        "outputId": "dcae60b3-a725-4b14-ca1b-ffe7c77651b7"
      },
      "source": [
        "params = {'beta1': 0.5, 'beta2': 0.999,'lr_g':0.0002,'lr_d':0.0002,'max_epochs':1}\n",
        "params['z_dim'] = 16\n",
        "params['base_size'] = 64\n",
        "\n",
        "gan = GAN(params)\n",
        "gan.train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([300, 1, 28, 28])\n",
            "torch.Size([300, 1, 28, 28])\n",
            "training start!!\n",
            "Epoch: 1\n",
            "x_ torch.Size([300, 1, 28, 28]) self.y_real_ torch.Size([300, 1, 28, 28])\n",
            "D_real torch.Size([300, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-31b2fa4f4626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-1ea3c86d8f46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mG_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mD_fake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_fake_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# 3. Do back propagation to compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_real_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mD_fake_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2753\u001b[0m         raise ValueError(\n\u001b[1;32m   2754\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m         )\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([300, 1, 28, 28])) that is different to the input size (torch.Size([300, 28])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    }
  ]
}